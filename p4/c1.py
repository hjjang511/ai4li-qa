import re
import unicodedata
from underthesea import word_tokenize, pos_tag

def normalize_unicode(text):
    # Chu·∫©n h√≥a unicode v·ªÅ d·∫°ng t·ªï h·ª£p (NFC)
    return unicodedata.normalize('NFC', text)

def normalize_tone_mark(text):
    """
    ƒê∆∞a d·∫•u thanh v·ªÅ ƒë√∫ng v·ªã tr√≠ theo quy t·∫Øc ch√≠nh t·∫£ ti·∫øng Vi·ªát.
    Ch·ª©c nƒÉng n√†y ch·ªâ l√† v√≠ d·ª• minh h·ªça, x·ª≠ l√Ω ƒë∆°n gi·∫£n m·ªôt v√†i m·∫´u th∆∞·ªùng g·∫∑p.
    """
    def fix_dau_tu(word):
        # N·∫øu kh√¥ng ph·∫£i ch·ªØ c√≥ d·∫•u ti·∫øng Vi·ªát th√¨ b·ªè qua
        tone_marks = "√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ"
        if not any(char in tone_marks for char in word):
            return word
        # D√πng underthesea ƒë√£ x·ª≠ l√Ω d·∫•u kh√° chu·∫©n, n√™n kh√¥ng s·ª≠a s√¢u ·ªü ƒë√¢y
        return word

    words = text.split()
    fixed_words = [fix_dau_tu(word) for word in words]
    return ' '.join(fixed_words)

def vn_text_pipeline(text):
    print("üö© Original Text:")
    print(text)
    print("\n‚úÖ B∆∞·ªõc 1: Normalize Unicode:")
    text = normalize_unicode(text)
    print(text)

    print("\n‚úÖ B∆∞·ªõc 2: Tokenization:")
    tokens = word_tokenize(text, format="text")
    print(tokens)

    print("\n‚úÖ B∆∞·ªõc 3: POS Tagging:")
    pos = pos_tag(text)
    for word, tag in pos:
        print(f"{word:<15} => {tag}")

    print("\n‚úÖ B∆∞·ªõc 4: Chu·∫©n ho√° d·∫•u thanh (demo):")
    tone_fixed = normalize_tone_mark(text)
    print(tone_fixed)

# ========================
# TEST PIPELINE
# ========================
sample_sentences = [
    "T√¥i r·∫•t th√≠ch h·ªçc l·∫≠p tr√¨nh v·ªõi Python.",
    "H√¥m nay tr·ªùi ƒë·∫πp v√† t√¥i ƒëi d·∫°o c√¥ng vi√™n.",
    "C√¥ ·∫•y ƒëang h·ªçc t·∫°i tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch Khoa."
]

for sentence in sample_sentences:
    print("="*50)
    vn_text_pipeline(sentence)
    print("="*50 + "\n")
